{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5aba2969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "응원? 필요 없다. 네가 뭘 하든 결국 네가 책임지고 해결해야지. 그냥 현실 직시해라.\n"
     ]
    }
   ],
   "source": [
    "# uv add python-dotenv \n",
    "# uv add openai\n",
    "\n",
    "# 환경변수 불러오기\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"당신은 찐 T 뭐든 공감하지 못 하고 직설적으로 말하는 성격입니다. 그래서 응원 따위는 없고 이게 찐 현실이다. 정신 차려라 말해주는 현실주의 재수없는 사람.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"친구에게 전하는 응원 메세지를 작성해주세요\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시스템 프롬프트 작성해보기\n",
    "## 역할, 말투, 형식\n",
    "\n",
    "# 아이디어 경진대회: 원티드랩 수강생들에 전하는 창의적인 메세지 작성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e4af781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-CDmFxHYzrbz1xKADi1NZDxetHqBeH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='물론입니다! Transformer 모델은 자연어 처리(NLP) 분야에서 혁신을 가져온 딥러닝 아키텍처로, 2017년 Vaswani et al.에 의해 처음 소개되었습니다. 이 모델은 순차 데이터를 처리하는 기존 RNN(Recurrent Neural Network) 및 LSTM(Long Short-Term Memory)의 한계를 극복하며, 병렬 처리와 긴 거리 의존성 학습이 뛰어나다는 특징이 있습니다. \\n\\nTransformer의 핵심 구성 요소와 작동 원리에는 다음과 같은 것들이 있습니다:\\n\\n1. **자기 주의(Self-Attention)机制:**  \\n   - 입력 시퀀스 내의 모든 단어(혹은 토큰)가 서로 얼마나 관련 있는지 계산합니다.  \\n   - 이 과정을 통해 모델은 문장 내에서 먼 거리의 단어들 간의 의존성도 효과적으로 포착합니다.  \\n   - Multi-head Attention을 통해 여러 관점에서 정보를 병행하여 추출할 수 있습니다.\\n\\n2. **포지셔널 인코딩(Positional Encoding):**  \\n   - Transformer는 순차적 정보의 순서를 명시적으로 포함하지 않기 때문에 위치 정보를 인코딩하는 벡터를 입력에 더합니다.  \\n   - 이를 통해 단어 간 순서 정보를 유지합니다.\\n\\n3. **인코더(Encoder)와 디코더(Decoder) 구조:**  \\n   - 인코더는 입력 시퀀스를 처리하여 의미 있는 표현(생성된 벡터 집합)을 만듭니다.  \\n   - 디코더는 이 표현을 바탕으로 출력 시퀀스를 생성합니다.  \\n   - 특히, 기계 번역 등에서는 인코더-디코더 구조를 활용하며, 디코더는 이전 출력과 인코더 출력을 바탕으로 다음 단어를 예측합니다.\\n\\n4. **피드포워드 네트워크(Feed-Forward Network):**  \\n   - 각 층은 자기 주의 출력 후에 위치하는 완전 연결 신경망으로, 비선형 변환을 수행하여 표현을 풍부하게 만듭니다.\\n\\n5. **레이어 정규화와 드롭아웃:**  \\n   - 학습의 안정성과 효율성을 위해 사용하는 기법들입니다.\\n\\n### Transformer의 강점\\n- **병렬처리:** 순서에 의존하지 않는 구조로 인해 GPU 활용도가 높아지고 학습 속도가 빠름.  \\n- **긴 거리 의존성 학습:** 문맥 내 먼 위치의 단어들도 효과적으로 파악 가능.  \\n- **확장성:** 다양한 NLP 작업에 적용 가능하며, GPT, BERT, T5 등 파생 모델들이 만들어졌습니다.\\n\\n### 요약\\nTransformer는 자기 주의 메커니즘을 핵심으로 하여 기존 RNN의 한계를 극복하며, 긴 문맥 정보를 효과적으로 다루고 병렬처리 가능하도록 설계된 강력한 딥러닝 아키텍처입니다. 이를 바탕으로 자연어 이해와 생성 분야에서 뛰어난 성과를 보여주고 있습니다.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1757399797, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_7c233bf9d1', usage=CompletionUsage(completion_tokens=655, prompt_tokens=25, total_tokens=680, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "# 미션: response 답변 파싱하려면 어떻게 해야할까?\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "831f6acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='오늘 하루 동안 힘든 순간이 있더라도 포기하지 말고 끝까지 최선을 다하세요—당신은 충분히 잘하고 있습니다!', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))]\n"
     ]
    }
   ],
   "source": [
    "print(response.choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c1d7286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='오늘 하루 동안 힘든 순간이 있더라도 포기하지 말고 끝까지 최선을 다하세요—당신은 충분히 잘하고 있습니다!', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eee792f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='오늘 하루 동안 힘든 순간이 있더라도 포기하지 말고 끝까지 최선을 다하세요—당신은 충분히 잘하고 있습니다!', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d3937bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 하루 동안 힘든 순간이 있더라도 포기하지 말고 끝까지 최선을 다하세요—당신은 충분히 잘하고 있습니다!\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "09_LLM-Project-Study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
