{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c22b8f0",
   "metadata": {},
   "source": [
    "# Î™®Îç∏ Î∂àÎü¨Ïò§Í≥† Ï∂îÎ°†ÌïòÍ∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f6b95e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv add huggingface_hub\n",
    "# ÌôòÍ≤ΩÎ≥ÄÏàò Î∂àÎü¨Ïò§Í∏∞\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "\n",
    "from huggingface_hub import login \n",
    "import os \n",
    "login(token=os.getenv(\"HUGGINGFACE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "619a8e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29d8979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"google/gemma-3-1b-pt\", device=\"cuda\", torch_dtype=torch.bfloat16)\n",
    "output = pipe(\"Eiffel tower is located in\", max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7dfa74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Eiffel tower is located in the heart of Paris.A great view of the Eiffel towerThe view from the top of the Eiffel towerThe view of the Eiffel tower from the bridge\\n\\nView of the Eiffel tower from the bridge\\n\\nThe view of the Eiffel tower from'}]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b293f7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345bbb95e20e4f87a7b345f91354c3f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--google--gemma-3-1b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38d830c3828478ab05a1058e2292bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b0e35e73894dbfb2da8bac5bcdce0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8243662711a446caaf83557e73877880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fde819300642fb999ea74988d496ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985df9681d8f41c3884bde70ecf57a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357fa1e5007140c89334e8e66a220edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa880ca0fe6c48dc91c78aebdcfef481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"google/gemma-3-1b-it\", device=\"cuda\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebbfcc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'generated_text': [{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a helpful assistant.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Write a poem on Hugging Face, the company'}]}, {'role': 'assistant', 'content': \"Okay, here's a poem about Hugging Face, aiming to capture its essence and feel:\\n\\n**The Neural Forge**\\n\\nA cloud of code, a vibrant hue,\\nHugging Face, a digital view.\\nA hub of models\"}]}]]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced701c",
   "metadata": {},
   "source": [
    "# 1) Gemma3-1B-it Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717009f8",
   "metadata": {},
   "source": [
    "### pipelineÏúºÎ°ú Î∂àÎü¨Ïò§Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c708fa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': [{'role': 'system', 'content': [{'type': 'text', 'text': 'ÏπúÏ†àÌïòÍ≤å ÎßêÌïòÏÑ∏Ïöî'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Ïò§Îäò ÌïòÎ£®Î•º ÏùëÏõêÌï¥Ï£ºÏÑ∏Ïöî'}]}, {'role': 'assistant', 'content': 'ÏïàÎÖïÌïòÏÑ∏Ïöî! üòä Ïò§Îäò ÌïòÎ£®Î•º ÏùëÏõêÌï¥ ÎìúÎ¶¥Í≤åÏöî. \\n\\nÌûòÎì† ÌïòÎ£®ÏòÄÏùÑ ÌÖêÎç∞, Ïû†Ïãú Ïà®ÏùÑ Í≥†Î•¥Í≥†, Ïò§Îäò ÌïòÎ£®Î•º Ï°∞Í∏à Îçî ÎÇòÏïÑÏßà Ïàò ÏûàÎèÑÎ°ù Í≤©Î†§Ìï¥ Ï£ºÍ≥† Ïã∂Ïñ¥Ïöî. \\n\\n*'}]}]\n"
     ]
    }
   ],
   "source": [
    "# uv add transformers\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# ÌååÏù¥ÌîÑÎùºÏù∏ ÎßåÎì§Í∏∞\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/gemma-3-1b-it\",\n",
    "    device=device,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# ÏûÖÎ†•Í∞í ÎßåÎì§Í∏∞\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"ÏπúÏ†àÌïòÍ≤å ÎßêÌïòÏÑ∏Ïöî\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Ïò§Îäò ÌïòÎ£®Î•º ÏùëÏõêÌï¥Ï£ºÏÑ∏Ïöî\"\n",
    "    }\n",
    "]\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"ÏπúÏ†àÌïòÍ≤å ÎßêÌïòÏÑ∏Ïöî\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"Ïò§Îäò ÌïòÎ£®Î•º ÏùëÏõêÌï¥Ï£ºÏÑ∏Ïöî\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=50)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e82aab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ï†ïÎ¶¨ 1. outputÏùÄ Ïñ¥ÎñªÍ≤å ÏÉùÍ≤ºÏùÑÍπå?\n",
    "# = list ÌòïÌÉúÎ°ú ÏÉùÍ≤ºÏúºÎ©∞ ÌååÏã±ÏùÑ Ìï† ÎïåÎäî Ïù∏Îç±Ïã±ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨Ïïº ÌïúÎã§.\n",
    "\n",
    "# Ï†ïÎ¶¨ 2. messages ÌòïÌÉúÎäî Ïôú Ïù¥Î†áÍ≤å Ïù¥Î£®Ïñ¥Ï†∏ ÏûàÏùÑÍπå?\n",
    "# = ÌÖçÏä§Ìä∏ÏôÄ Ïù¥ÎØ∏ÏßÄÎ•º Î™®Îëê Ï∂úÎ†• Ìï† Ïàò ÏûàÎäî Î©ÄÌã∞Î™®Îç∏Ïù¥Í∏∞ ÎïåÎ¨∏ÏûÖÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac6dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': [{'role': 'system', 'content': [{'type': 'text', 'text': 'ÏπúÏ†àÌïòÍ≤å ÎßêÌïòÏÑ∏Ïöî.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Ïò§Îäò ÌïòÎ£®Î•º ÏùëÏõêÌï¥Ï£ºÏÑ∏Ïöî.'}]}, {'role': 'assistant', 'content': 'ÏïàÎÖïÌïòÏÑ∏Ïöî! Ïò§Îäò ÌïòÎ£®ÎèÑ ÏãúÏûëÌïòÎäî ÎãπÏã†ÏóêÍ≤å Îî∞ÎúªÌïú Í≤©Î†§ÏôÄ ÏùëÏõêÏùÑ Ï†ÑÌïòÍ≥† Ïã∂ÎÑ§Ïöî. \\n\\nÌûòÎì† ÏùºÏù¥ ÏûàÍ±∞ÎÇò, Í∏çÏ†ïÏ†ÅÏù∏ Í∏∞Ïö¥Ïù¥ Îì§ÏßÄ ÏïäÍ±∞ÎÇò, ÌòπÏùÄ Í∑∏ÎÉ• ÌïòÎ£®Î•º ÏãúÏûëÌïòÍ∏∞ ÏúÑÌï¥ ÌäπÎ≥Ñ'}]}\n",
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'ÏπúÏ†àÌïòÍ≤å ÎßêÌïòÏÑ∏Ïöî.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Ïò§Îäò ÌïòÎ£®Î•º ÏùëÏõêÌï¥Ï£ºÏÑ∏Ïöî.'}]}, {'role': 'assistant', 'content': 'ÏïàÎÖïÌïòÏÑ∏Ïöî! Ïò§Îäò ÌïòÎ£®ÎèÑ ÏãúÏûëÌïòÎäî ÎãπÏã†ÏóêÍ≤å Îî∞ÎúªÌïú Í≤©Î†§ÏôÄ ÏùëÏõêÏùÑ Ï†ÑÌïòÍ≥† Ïã∂ÎÑ§Ïöî. \\n\\nÌûòÎì† ÏùºÏù¥ ÏûàÍ±∞ÎÇò, Í∏çÏ†ïÏ†ÅÏù∏ Í∏∞Ïö¥Ïù¥ Îì§ÏßÄ ÏïäÍ±∞ÎÇò, ÌòπÏùÄ Í∑∏ÎÉ• ÌïòÎ£®Î•º ÏãúÏûëÌïòÍ∏∞ ÏúÑÌï¥ ÌäπÎ≥Ñ'}]\n",
      "{'role': 'system', 'content': [{'type': 'text', 'text': 'ÏπúÏ†àÌïòÍ≤å ÎßêÌïòÏÑ∏Ïöî.'}]}\n",
      "{'role': 'user', 'content': [{'type': 'text', 'text': 'Ïò§Îäò ÌïòÎ£®Î•º ÏùëÏõêÌï¥Ï£ºÏÑ∏Ïöî.'}]}\n",
      "{'role': 'assistant', 'content': 'ÏïàÎÖïÌïòÏÑ∏Ïöî! Ïò§Îäò ÌïòÎ£®ÎèÑ ÏãúÏûëÌïòÎäî ÎãπÏã†ÏóêÍ≤å Îî∞ÎúªÌïú Í≤©Î†§ÏôÄ ÏùëÏõêÏùÑ Ï†ÑÌïòÍ≥† Ïã∂ÎÑ§Ïöî. \\n\\nÌûòÎì† ÏùºÏù¥ ÏûàÍ±∞ÎÇò, Í∏çÏ†ïÏ†ÅÏù∏ Í∏∞Ïö¥Ïù¥ Îì§ÏßÄ ÏïäÍ±∞ÎÇò, ÌòπÏùÄ Í∑∏ÎÉ• ÌïòÎ£®Î•º ÏãúÏûëÌïòÍ∏∞ ÏúÑÌï¥ ÌäπÎ≥Ñ'}\n"
     ]
    }
   ],
   "source": [
    "# ÎØ∏ÏÖò output ÌååÏã±Ìï¥Î≥¥Í∏∞\n",
    "for out in output:\n",
    "    print(out)\n",
    "    print(out[\"generated_text\"])\n",
    "    for x in out[\"generated_text\"]:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c1e95e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'generated_text': [{'role': 'system', 'content': [{'type': 'text', 'text': 'ÏπúÏ†àÌïòÍ≤å ÎßêÌïòÏÑ∏Ïöî'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Ïò§Îäò ÌïòÎ£®Î•º ÏùëÏõêÌï¥Ï£ºÏÑ∏Ïöî'}]}, {'role': 'assistant', 'content': 'ÏïàÎÖïÌïòÏÑ∏Ïöî! Ïò§Îäò ÌïòÎ£®Î•º ÏùëÏõêÌï¥ ÎìúÎ¶¥Í≤åÏöî. üòä \\n\\nÌûòÎì† ÌïòÎ£®ÏòÄÏùÑ ÌÖêÎç∞, Ïû†Ïãú Ïâ¨Ïñ¥Í∞ÄÎ©¥ÏÑú ÏûëÏùÄ Í∏∞Î∂Ñ Ï¢ãÏùÄ ÏùºÏù¥ÎùºÎèÑ Ï∞æÏïÑÎ≥¥ÏÑ∏Ïöî. \\n\\n*   **Í∏çÏ†ïÏ†ÅÏù∏ ÏÉùÍ∞Å:** Ïò§Îäò'}]}]]\n"
     ]
    }
   ],
   "source": [
    "# ÏûÖÎ†•Í∞í ÎßåÎì§Í∏∞\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"ÏπúÏ†àÌïòÍ≤å ÎßêÌïòÏÑ∏Ïöî\"}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Ïò§Îäò ÌïòÎ£®Î•º ÏùëÏõêÌï¥Ï£ºÏÑ∏Ïöî\"}]\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=50)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b6952",
   "metadata": {},
   "source": [
    "### Î™®Îç∏Î°ú Î∂àÎü¨Ïò§Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3674e506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<bos><start_of_turn>user\\nYou are a helpful assistant.\\n\\nWrite a poem on Hugging Face, the company<end_of_turn>\\n<start_of_turn>model\\nOkay, here's a poem about Hugging Face, aiming for a balance of admiration and a touch of the technical feel:\\n\\n---\\n\\nThe Model Maze, a vibrant hue,\\nHugging Face, a landscape new.\\nA hub of minds, a coding grace,\\nConnecting models, time and space.\"]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "import torch\n",
    "# Î™®Îç∏ Ïù¥Î¶Ñ ÏÑ§Ï†ï\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# STEP1. Î™®Îç∏, ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î∂àÎü¨Ïò§Í∏∞\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# STEP2. ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑÌïòÍ∏∞\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "# STEP3. ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ ÌÜ†ÌÅ¨ÎÇòÏù¥ÏßïÌïòÍ∏∞\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(device)\n",
    "\n",
    "# STEP4. Ï∂îÎ°†ÌïòÍ∏∞\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "outputs = tokenizer.batch_decode(outputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4838a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\"device\": \"cuda\", \"verbose\": True}\n",
    "\n",
    "temp_func(**test_dict)\n",
    "\n",
    "temp_func(device=\"cuda\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86859026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv add accelerate\n",
    "# uv add bitsandbytes\n",
    "\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "import torch\n",
    "# Î™®Îç∏ Ïù¥Î¶Ñ ÏÑ§Ï†ï\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# STEP1. Î™®Îç∏, ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î∂àÎü¨Ïò§Í∏∞\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c5019e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2,    105,   2364,    107,   3048,    659,    496,  11045,  16326,\n",
      "         236761,    108,   6974,    496,  27355,    580,  22798,   3801,   7117,\n",
      "         236764,    506,   2544,    106,    107,    105,   4368,    107]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# STEP2. ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑÌïòÍ∏∞\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "# STEP3. ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ ÌÜ†ÌÅ¨ÎÇòÏù¥ÏßïÌïòÍ∏∞\n",
    "# ÌÖåÏä§Ìä∏Ìï¥Î≥¥Í∏∞ 1. (add_generation_prompt=False, tokenize=False)\n",
    "# ÌÖåÏä§Ìä∏Ìï¥Î≥¥Í∏∞ 2. (add_generation_prompt=True, tokenize=False)\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True, # inputÎí§ÏóêÎã§Í∞Ä assistantÎ•º Î∂ôÏùºÏßÄ Í≤∞Ï†ï\n",
    "    tokenize=True,              # Í≤∞Í≥ºÎ•º ÌÜ†ÌÅ∞ÌôîÌï†ÏßÄ Ïó¨Î∂Ä\n",
    "    return_dict=True,           # Í≤∞Í≥ºÎ•º ÎîïÏÖîÎÑàÎ¶¨Î°ú Î∞òÌôòÌïòÍ≤å Ìï† Í≤ÉÏù∏ÏßÄÏùò Ïó¨Î∂Ä\n",
    "    return_tensors=\"pt\",        # Í≤∞Í≥ºÎ•º ÌååÏù¥ÌÜ†Ïπò ÌòïÏãùÏúºÎ°ú Î∞òÌôòÌï† Í≤ÉÏù∏ÏßÄÏùò Ïó¨Î∂Ä\n",
    ")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a61f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ï†ïÎ¶¨1. messagesÎ≥ÄÏàòÍ∞Ä ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ•º ÎßåÎÇò Ïà´ÏûêÎ°ú Î∞îÎÄåÎäî Í≥ºÏ†ïÏùÄ Ïñ¥Îñ§Í∞Ä?\n",
    "messages = [\n",
    "    {},\n",
    "    {}\n",
    "]\n",
    "# Ï†ïÎ¶¨2. Ïö∞Î¶¨Í∞Ä ÏòàÏ∏°ÏùÑ ÌïòÍ∏∞ ÏúÑÌï¥ÏÑúÎäî Ïñ¥Îñ§ Îç∞Ïù¥ÌÑ∞Í∞Ä Ï§ÄÎπÑÎêòÏñ¥Ïïº Ìï†Íπå?\n",
    "# Ï†ïÎ¶¨3. ÏòàÏ∏°ÏùÑ ÌïòÎäî Í≥ºÏ†ïÏóêÏÑú \"**\"Îäî Ïôú Ïì∞ÎäîÍ±∏Íπå?\n",
    "# Ï†ïÎ¶¨4. outputÏùÄ Ïñ¥ÎñªÍ≤å ÎÇòÏò§ÎäîÍ∞Ä?\n",
    "# Ï†ïÎ¶¨5. ouutputÏóêÏÑú ÎãµÎ≥ÄÏùÄ Ïñ¥ÎñªÍ≤å Ï∂îÏ∂úÌï† Ïàò ÏûàÏùÑÍπå?\n",
    "\n",
    "## messagesÎ•º ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ•º ÌÜµÌï¥ Ïà´ÏûêÎ°ú Î∞îÍøîÏïº ÌïúÎã§!\n",
    "## tokenizer.apply_chat_templateÏù¥ÎùºÎäî Ìï®ÏàòÎ•º Ïù¥Ïö©, tokenize=False ---- messagesÍ∞Ä Î¨∏ÏûêÎ°ú Î∞îÎÄê ÏÉÅÌÉúÍ∞Ä ÎÇòÏò®Îã§.\n",
    "## tokenize=TrueÎ°ú ÏÑ§Ï†ïÌïòÍ≤å ÎêòÎ©¥, input_ids, attention_mask ÎîïÏÖîÎÑàÎ¶¨Î°ú Ï∂úÎ†•ÎêúÎã§.\n",
    "## input_ids : Î¨∏Ïûê -> Ïà´Ïûê , attention_mask: Í∑∏ ÏûêÎ¶¨Í∞Ä ÏùòÎØ∏Í∞Ä ÏûàÎäî ÏûêÎ¶¨Ïù∏ÏßÄ ÏùòÎØ∏Í∞Ä ÏóÜÎäî ÏûêÎ¶¨Ïù∏ÏßÄÎ•º ÏïåÎ†§Ï§å\n",
    "## Ïù¥Ï†ú inputsÍ∞Ä Ï§ÄÎπÑÎêòÏóàÎã§ -- GPUÏóê ÏòÆÍ∏¥Îã§ to(device) -- model.generate()\n",
    "## outputsÍ∞Ä ÎÇòÏò®Îã§. Ïù¥ ÏπúÍµ¨Îäî Ïñ¥ÎñªÍ≤å ÏÉùÍ≤ºÏùÑÍπå? ÎãµÎ≥ÄÏù¥ Ïñ¥ÎîîÏûàÎäîÏßÄ Ï∞æÏùÑ Ïàò ÏûàÎäîÍ∞Ä??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484b3b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=False, # inputÎí§ÏóêÎã§Í∞Ä assistantÎ•º Î∂ôÏùºÏßÄ Í≤∞Ï†ï\n",
    "    tokenize=False,              # Í≤∞Í≥ºÎ•º ÌÜ†ÌÅ∞ÌôîÌï†ÏßÄ Ïó¨Î∂Ä\n",
    ")\n",
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10a87abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True, # inputÎí§ÏóêÎã§Í∞Ä assistantÎ•º Î∂ôÏùºÏßÄ Í≤∞Ï†ï\n",
    "    tokenize=False,              # Í≤∞Í≥ºÎ•º ÌÜ†ÌÅ∞ÌôîÌï†ÏßÄ Ïó¨Î∂Ä\n",
    ")\n",
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56654c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2,    105,   2364,    107,   3048,    659,    496,  11045,  16326,\n",
       "         236761,    108,   6974,    496,  27355,    580,  22798,   3801,   7117,\n",
       "         236764,    506,   2544,    106,    107,    105,   4368,    107]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f88c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(input_ids, attention_mask, max_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c0feef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<bos><start_of_turn>user\\nYou are a helpful assistant.\\n\\nWrite a poem on Hugging Face, the company<end_of_turn>\\n<start_of_turn>model\\nOkay, here's a poem about Hugging Face, aiming to capture its essence and impact:\\n\\n**The Neural Heart of the Web**\\n\\nA name whispered, a vibrant hue,\\nHugging Face, a digital view.\\nNo dusty servers, cold and deep,\\nBut models blooming, secrets to keep.\\n\\nFrom Transformers bright and bold,\\nTo datasets vast, a story told.\\nA community of minds so keen,\\nBuilding neural networks, a vibrant scene.\\n\\nWith\"]\n"
     ]
    }
   ],
   "source": [
    "# STEP4. Ï∂îÎ°†ÌïòÍ∏∞\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "outputs = tokenizer.batch_decode(outputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fbb20e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Okay, here‚Äôs a poem about Hugging Face, aiming for a warm and appreciative tone:\n",
      "\n",
      "**The Neural Heart of the Web**\n",
      "\n",
      "A place of models, vast and bright,\n",
      "Hugging Face, a digital light.\n",
      "A hub of code, a vibrant scene,\n",
      "Where AI‚Äôs wonders\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e23b1b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2,    105,   2364,    107,   3048,    659,    496,  11045,  16326,\n",
       "         236761,    108,   6974,    496,  27355,    580,  22798,   3801,   7117,\n",
       "         236764,    506,   2544,    106,    107,    105,   4368,    107]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1bd3fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ou are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Okay, here‚Äôs a poem about Hugging Face, aiming for a warm and appreciative tone:\n",
      "\n",
      "**The Neural Heart of the Web**\n",
      "\n",
      "A place of models, vast and bright,\n",
      "Hugging Face, a digital light.\n",
      "A hub of code, a vibrant scene,\n",
      "Where AI‚Äôs wonders\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True, # inputÎí§ÏóêÎã§Í∞Ä assistantÎ•º Î∂ôÏùºÏßÄ Í≤∞Ï†ï\n",
    "    tokenize=True,              # Í≤∞Í≥ºÎ•º ÌÜ†ÌÅ∞ÌôîÌï†ÏßÄ Ïó¨Î∂Ä\n",
    "    return_dict=True,           # Í≤∞Í≥ºÎ•º ÎîïÏÖîÎÑàÎ¶¨Î°ú Î∞òÌôòÌïòÍ≤å Ìï† Í≤ÉÏù∏ÏßÄÏùò Ïó¨Î∂Ä\n",
    "    return_tensors=\"pt\",        # Í≤∞Í≥ºÎ•º ÌååÏù¥ÌÜ†Ïπò ÌòïÏãùÏúºÎ°ú Î∞òÌôòÌï† Í≤ÉÏù∏ÏßÄÏùò Ïó¨Î∂Ä\n",
    ")\n",
    "input_len = inputs[\"input_ids\"].shape[1]\n",
    "print(outputs[0][input_len:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ffc7bf",
   "metadata": {},
   "source": [
    "## 2) Gemma3-1B-pt Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667e0f83",
   "metadata": {},
   "source": [
    "### pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c6c0c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"google/gemma-3-1b-pt\", \n",
    "    device=device, \n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "output = pipe(\"Eiffel tower is located in\", max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c0df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)\n",
    "print(output[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008086a6",
   "metadata": {},
   "source": [
    "### Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad14c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode():\n\u001b[32m     28\u001b[39m     outputs = model.generate(\n\u001b[32m     29\u001b[39m         **inputs, \n\u001b[32m     30\u001b[39m         max_new_tokens=\u001b[32m50\u001b[39m, \n\u001b[32m     31\u001b[39m         do_sample=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     32\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m outputs = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3897\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   3894\u001b[39m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[32m   3895\u001b[39m token_ids = to_py_obj(token_ids)\n\u001b[32m-> \u001b[39m\u001b[32m3897\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3899\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3900\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3901\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3902\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:682\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    681\u001b[39m     token_ids = [token_ids]\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    684\u001b[39m clean_up_tokenization_spaces = (\n\u001b[32m    685\u001b[39m     clean_up_tokenization_spaces\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    687\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.clean_up_tokenization_spaces\n\u001b[32m    688\u001b[39m )\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[31mTypeError\u001b[39m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM\n",
    "# Î™®Îç∏ Ïù¥Î¶Ñ ÏÑ§Ï†ï\n",
    "model_id = \"google/gemma-3-1b-pt\"\n",
    "\n",
    "# STEP1. Î™®Îç∏, ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î∂àÎü¨Ïò§Í∏∞\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# STEP2. ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑÌïòÍ∏∞\n",
    "prompt = \"Eiffel tower is located in\"\n",
    "\n",
    "# STEP3. ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ ÌÜ†ÌÅ¨ÎÇòÏù¥ÏßïÌïòÍ∏∞\n",
    "inputs = tokenizer(\n",
    "    prompt, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "inputs = inputs.to(device) # GPU Î≥¥ÎÇ¥Í∏∞\n",
    "\n",
    "# STEP4. Ï∂îÎ°†ÌïòÍ∏∞\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=50, \n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "outputs = tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aad38752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM\n",
    "# Î™®Îç∏ Ïù¥Î¶Ñ ÏÑ§Ï†ï\n",
    "model_id = \"google/gemma-3-1b-pt\"\n",
    "\n",
    "# STEP1. Î™®Îç∏, ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î∂àÎü¨Ïò§Í∏∞\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c13ec3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2, 236788,  80880,  18515,    563,   5628,    528]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# STEP2. ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑÌïòÍ∏∞\n",
    "prompt = \"Eiffel tower is located in\"\n",
    "\n",
    "# STEP3. ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ ÌÜ†ÌÅ¨ÎÇòÏù¥ÏßïÌïòÍ∏∞\n",
    "inputs = tokenizer(\n",
    "    prompt, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(inputs)\n",
    "inputs = inputs.to(device) # GPU Î≥¥ÎÇ¥Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b42edd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2, 236788,  80880,  18515,    563,   5628,    528,    506,   3710,\n",
      "            529,   9079, 236764,   7001, 236761, 255999,    818,  94648,  25822,\n",
      "            563,    496, 236743, 236800, 236778, 236812, 236772,  33307, 236772,\n",
      "          11480,  18515,    528,   9079, 236764,   7001, 236761, 255999,    818,\n",
      "          94648,  25822,    563,    496,   5404,    529,   9079,    532,   7001,\n",
      "         236761, 255999,    818,  94648,  25822,    563,    496,   5404,    529,\n",
      "           9079,    532,   7001]], device='cuda:0')\n",
      "torch.Size([1, 57])\n",
      "tensor([     2, 236788,  80880,  18515,    563,   5628,    528,    506,   3710,\n",
      "           529,   9079, 236764,   7001, 236761, 255999,    818,  94648,  25822,\n",
      "           563,    496, 236743, 236800, 236778, 236812, 236772,  33307, 236772,\n",
      "         11480,  18515,    528,   9079, 236764,   7001, 236761, 255999,    818,\n",
      "         94648,  25822,    563,    496,   5404,    529,   9079,    532,   7001,\n",
      "        236761, 255999,    818,  94648,  25822,    563,    496,   5404,    529,\n",
      "          9079,    532,   7001], device='cuda:0')\n",
      "<bos>Eiffel tower is located in the heart of Paris, France.<start_of_image>The Eiffel Tower is a 324-meter-high tower in Paris, France.<start_of_image>The Eiffel Tower is a symbol of Paris and France.<start_of_image>The Eiffel Tower is a symbol of Paris and France\n"
     ]
    }
   ],
   "source": [
    "# STEP4. Ï∂îÎ°†ÌïòÍ∏∞\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=50, \n",
    "        do_sample=False\n",
    "    )\n",
    "print(outputs)\n",
    "print(outputs.shape)\n",
    "print(outputs[0])\n",
    "\n",
    "output = tokenizer.decode(outputs[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54367f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>Eiffel tower is located in the heart of Paris, France.<start_of_image>The Eiffel Tower is a 324-meter-high tower in Paris, France.<start_of_image>The Eiffel Tower is a symbol of Paris and France.<start_of_image>The Eiffel Tower is a symbol of Paris and France']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f9412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ï†ïÎ¶¨\n",
    "# 1. Ïö∞Î¶¨Í∞Ä ÌóàÍπÖÌéòÏù¥Ïä§ÏóêÏÑú LLM Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÎ†§Í≥† Ìï† Îïå, Î™®Îç∏ÏùÄ Ïñ¥ÎñªÍ≤å Î∂àÎü¨Ïò§ÎÇòÏöî?\n",
    "# 2. instruct Î™®Îç∏Ïù¥ ÏûàÍ≥†, Í∑∏ÎÉ• pre-trained Î™®Îç∏Ïù¥ ÏûàÎäîÎç∞ Î™®Îç∏Ïóê inputÌï¥Ïïº Ìï† Îç∞Ïù¥ÌÑ∞Îäî Ïñ¥ÎñªÍ≤å ÏÉùÍ≤ºÎÇòÏöî?\n",
    "# 3. input Ìï¥ÏïºÌï† Îç∞Ïù¥ÌÑ∞Îäî Ïñ¥ÎñªÍ≤å ÎßåÎìúÎÇòÏöî?\n",
    "# 4. modelÏóêÏÑú input dataÎ•º ÎÑ£ÏùÄ Îã§Ïùå ÎÇòÏò® outputÏùÄ Ïñ¥ÎñªÍ≤å ÏÉùÍ≤ºÎÇòÏöî?\n",
    "# 5. outputÏùÑ ÌÖçÏä§Ìä∏Î°ú Î∞îÍæ∏Î†§Î©¥ Ïñ¥ÎñªÍ≤å Ìï¥Ïïº ÌïòÎÇòÏöî?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c271212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19b01590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2, 239120, 238500, 238503, 239592, 237170, 110388, 237223,   5386,\n",
      "         103595, 236881,    107, 238136, 239484, 241845, 237456, 110388, 237223,\n",
      "           5386, 103595, 236881]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# ÏùëÏö©Ìï¥Î≥¥Í∏∞\n",
    "prompt = [\n",
    "    \"ÎÇ®ÏÇ∞ÌÉÄÏõåÎäî Ïñ¥ÎîîÏóê ÏûàÎÇòÏöî?<eos>\",\n",
    "    \"Í≤ΩÎ≥µÍ∂ÅÏùÄ Ïñ¥ÎîîÏóê ÏûàÎÇòÏöî?<eos>\"\n",
    "]\n",
    "prompt = \"ÎÇ®ÏÇ∞ÌÉÄÏõåÎäî Ïñ¥ÎîîÏóê ÏûàÎÇòÏöî?\\nÍ≤ΩÎ≥µÍ∂ÅÏùÄ Ïñ¥ÎîîÏóê ÏûàÎÇòÏöî?\"\n",
    "\n",
    "# STEP3. ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ ÌÜ†ÌÅ¨ÎÇòÏù¥ÏßïÌïòÍ∏∞\n",
    "inputs = tokenizer(\n",
    "    \"\".join(prompt), \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(inputs)\n",
    "inputs = inputs.to(device) # GPU Î≥¥ÎÇ¥Í∏∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98458ef9",
   "metadata": {},
   "source": [
    "### Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a403e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20eceb2c1fc042a98a01667bfaf97c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/710 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--MLP-KTLim--llama-3-Korean-Bllossom-8B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e7cb1b31464c149ab444bd08c40df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89cfaaaac6d14a82919cecdb1d65e0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c4a303aa4945f1a536198aeedbc734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571d11e96173482b97b4cb297a583f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba37e8c0263b4aacb642e7afc8132bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22245f6ffd7478f906c20d8d606270e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3e7e28db6a4972ac9cb2124c7f20cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f87d66487e41dd9764bebe8b5068f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24f7a7aac9b4d40bf55ccda43e9ebef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6be1b50d6e4e2ab57cb38581aedc92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5c59b4a77c42f59ce22ef61d05af17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     23\u001b[39m prompt = pipeline.tokenizer.apply_chat_template(\n\u001b[32m     24\u001b[39m         messages, \n\u001b[32m     25\u001b[39m         tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[32m     26\u001b[39m         add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     27\u001b[39m )\n\u001b[32m     29\u001b[39m terminators = [\n\u001b[32m     30\u001b[39m     pipeline.tokenizer.eos_token_id,\n\u001b[32m     31\u001b[39m     pipeline.tokenizer.convert_tokens_to_ids(\u001b[33m\"\u001b[39m\u001b[33m<|eot_id|>\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m outputs = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mterminators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\n\u001b[32m     41\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(outputs[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m][\u001b[38;5;28mlen\u001b[39m(prompt):])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:332\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    330\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1467\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1459\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1460\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1461\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1464\u001b[39m         )\n\u001b[32m   1465\u001b[39m     )\n\u001b[32m   1466\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1467\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1474\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1473\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1474\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1475\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1476\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1374\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1372\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1373\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:432\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    430\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    435\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2539\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2528\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m GenerationMixin.generate(\n\u001b[32m   2529\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2530\u001b[39m         inputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2534\u001b[39m         **kwargs,\n\u001b[32m   2535\u001b[39m     )\n\u001b[32m   2537\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2539\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2540\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2546\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2547\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2550\u001b[39m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[32m   2551\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._beam_search(\n\u001b[32m   2552\u001b[39m         input_ids,\n\u001b[32m   2553\u001b[39m         logits_processor=prepared_logits_processor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2557\u001b[39m         **model_kwargs,\n\u001b[32m   2558\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2870\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2868\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2869\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2870\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2872\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2873\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2874\u001b[39m     outputs,\n\u001b[32m   2875\u001b[39m     model_kwargs,\n\u001b[32m   2876\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2877\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:940\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    939\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    942\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:459\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    440\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    441\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    442\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    443\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    444\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 module.forward = make_capture_wrapper(module, original_forward, key, specs.index)\n\u001b[32m   1062\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[32m   1066\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:395\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    407\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    408\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    409\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:309\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m residual = hidden_states\n\u001b[32m    308\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:155\u001b[39m, in \u001b[36mLlamaMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28mself\u001b[39m.gate_proj(x)) * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:170\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_forward\u001b[39m(module, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     args, kwargs = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_hf_hook\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module._hf_hook.no_grad:\n\u001b[32m    172\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:360\u001b[39m, in \u001b[36mAlignDevicesHook.pre_forward\u001b[39m\u001b[34m(self, module, *args, **kwargs)\u001b[39m\n\u001b[32m    352\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    353\u001b[39m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    354\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    355\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m value.data_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map\n\u001b[32m    356\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map[value.data_ptr()]\n\u001b[32m    357\u001b[39m         ):\n\u001b[32m    358\u001b[39m             \u001b[38;5;28mself\u001b[39m.tied_pointers_to_remove.add((value.data_ptr(), \u001b[38;5;28mself\u001b[39m.execution_device))\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m.execution_device), send_to_device(\n\u001b[32m    370\u001b[39m     kwargs, \u001b[38;5;28mself\u001b[39m.execution_device, skip_keys=\u001b[38;5;28mself\u001b[39m.skip_keys\n\u001b[32m    371\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\POTENUP\\09_LLM-Study\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:343\u001b[39m, in \u001b[36mset_module_tensor_to_device\u001b[39m\u001b[34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map, non_blocking, clear_cache)\u001b[39m\n\u001b[32m    341\u001b[39m             module._parameters[tensor_name] = param_cls(new_value, requires_grad=old_value.requires_grad)\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     new_value = \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    345\u001b[39m     new_value = torch.tensor(value, device=device)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "pipeline.model.eval()\n",
    "\n",
    "PROMPT = '''You are a helpful AI assistant. Please answer the user's questions kindly. ÎãπÏã†ÏùÄ Ïú†Îä•Ìïú AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ ÏûÖÎãàÎã§. ÏÇ¨Ïö©ÏûêÏùò ÏßàÎ¨∏Ïóê ÎåÄÌï¥ ÏπúÏ†àÌïòÍ≤å ÎãµÎ≥ÄÌï¥Ï£ºÏÑ∏Ïöî.'''\n",
    "instruction = \"ÏÑúÏö∏Ïùò Ïú†Î™ÖÌïú Í¥ÄÍ¥ë ÏΩîÏä§Î•º ÎßåÎì§Ïñ¥Ï§ÑÎûò?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
    "    ]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=2048,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3a17db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gemme-3 1b it\n",
    "\n",
    "# Ï†ïÎ¶¨1. messagesÎ≥ÄÏàòÍ∞Ä ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ•º ÎßåÎÇò Ïà´ÏûêÎ°ú Î∞îÎÄåÎäî Í≥ºÏ†ïÏùÄ Ïñ¥Îñ§Í∞Ä?\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "## Î™®Îç∏Ïóê Î∞îÎ°ú ÎÑ£ÏùÑ Ïàò ÏûàÎäîÍ∞Ä????? No\n",
    "## messagesÎ•º Ïà´Ïûê ÌÖêÏÑúÎ°ú Î∞îÍøîÏïº ÌïúÎã§. ---- How?? tokenizer.apply_chat_template(message)\n",
    "## ÎßåÏïΩ tokenize=TrueÎ•º ÌïòÎ©¥, {\"input_ids\": , \"attention_mask\": }\n",
    "## ??? messagesÎäî Î¨∏ÏûêÏù¥ ÏïÑÎãåÎç∞ Ïñ¥ÎñªÍ≤å Ïà´ÏûêÎ°ú Î∞îÍæ∏ÎäîÍ±∞ÏßÄ?????? tokenize=False Ìï¥Î≥¥Î©¥ Ïïå Ïàò ÏûàÎã§.\n",
    "##  <bos><start.......\n",
    "\n",
    "# Ï†ïÎ¶¨2. Ïö∞Î¶¨Í∞Ä ÏòàÏ∏°ÏùÑ ÌïòÍ∏∞ ÏúÑÌï¥ÏÑúÎäî Ïñ¥Îñ§ Îç∞Ïù¥ÌÑ∞Í∞Ä Ï§ÄÎπÑÎêòÏñ¥Ïïº Ìï†Íπå?\n",
    "## inputs = {\"input_ids\": \"A\", \"attention_mask\": \"B\"}\n",
    "\n",
    "# Ï†ïÎ¶¨3. ÏòàÏ∏°ÏùÑ ÌïòÎäî Í≥ºÏ†ïÏóêÏÑú \"**\"Îäî Ïôú Ïì∞ÎäîÍ±∏Íπå?\n",
    "## def myfunc(input_ids, attention_mask, message=\"CCC\", verbose=False):\n",
    "## ....\n",
    "## myfunc(**inputs) Ïùò ÏùòÎØ∏Îäî myfunc(input_ids=\"A\", attention_mask=\"B\")Î°ú Ìï¥Ï£ºÏÑ∏ÏöîÏôÄ Í∞ôÎã§.\n",
    "\n",
    "# Ï†ïÎ¶¨4. outputÏùÄ Ïñ¥ÎñªÍ≤å ÎÇòÏò§ÎäîÍ∞Ä?\n",
    "## outputs = model(inputs)\n",
    "# outputs = tensor([[     2, 236788,  80880,  18515,    563,   5628,    528,    506,   3710,\n",
    "#             529,   9079, 236764,   7001, 236761, 255999,    818,  94648,  25822,\n",
    "#             563,    496, 236743, 236800, 236778, 236812, 236772,  33307, 236772,\n",
    "#           11480,  18515,    528,   9079, 236764,   7001, 236761, 255999,    818,\n",
    "#           94648,  25822,    563,    496,   5404,    529,   9079,    532,   7001,\n",
    "#          236761, 255999,    818,  94648,  25822,    563,    496,   5404,    529,\n",
    "#            9079,    532,   7001]], device='cuda:0')\n",
    "\n",
    "# Ï†ïÎ¶¨5. ouutputÏóêÏÑú ÎãµÎ≥ÄÏùÄ Ïñ¥ÎñªÍ≤å Ï∂îÏ∂úÌï† Ïàò ÏûàÏùÑÍπå?\n",
    "# decodeÎ•º ÌÜµÌï¥ÏÑú Ïà´Ïûê ÌÖêÏÑúÎ•º ÌÖçÏä§Ìä∏Î°ú Î∞îÍøîÏïº ÌïúÎã§. \n",
    "# tensor([Îç∞Ïù¥ÌÑ∞1, Îç∞Ïù¥ÌÑ∞2, ...]) Ïù∏ Í≤ΩÏö∞, tokenizer.batch_decode(outputs)\n",
    "# tensor(Îç∞Ïù¥ÌÑ∞1)Ïù∏ Í≤ΩÏö∞, tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63678a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gemma-3 1b pt\n",
    "\n",
    "# Ï†ïÎ¶¨\n",
    "# 1. Ïö∞Î¶¨Í∞Ä ÌóàÍπÖÌéòÏù¥Ïä§ÏóêÏÑú LLM Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÎ†§Í≥† Ìï† Îïå, Î™®Îç∏ÏùÄ Ïñ¥ÎñªÍ≤å Î∂àÎü¨Ïò§ÎÇòÏöî?\n",
    "# 2. instruct Î™®Îç∏Ïù¥ ÏûàÍ≥†, Í∑∏ÎÉ• pre-trained Î™®Îç∏Ïù¥ ÏûàÎäîÎç∞ Î™®Îç∏Ïóê inputÌï¥Ïïº Ìï† Îç∞Ïù¥ÌÑ∞Îäî Ïñ¥ÎñªÍ≤å ÏÉùÍ≤ºÎÇòÏöî?\n",
    "# 3. input Ìï¥ÏïºÌï† Îç∞Ïù¥ÌÑ∞Îäî Ïñ¥ÎñªÍ≤å ÎßåÎìúÎÇòÏöî?\n",
    "# 4. modelÏóêÏÑú input dataÎ•º ÎÑ£ÏùÄ Îã§Ïùå ÎÇòÏò® outputÏùÄ Ïñ¥ÎñªÍ≤å ÏÉùÍ≤ºÎÇòÏöî?\n",
    "# 5. outputÏùÑ ÌÖçÏä§Ìä∏Î°ú Î∞îÍæ∏Î†§Î©¥ Ïñ¥ÎñªÍ≤å Ìï¥Ïïº ÌïòÎÇòÏöî?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "09_LLM-Project-Study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
